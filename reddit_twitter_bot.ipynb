{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJ/H3+J90R/hkqTwx0V6pA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bharath-kalkote23/reddit-twitter-Bot/blob/master/reddit_twitter_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1"
      ],
      "metadata": {
        "id": "qKbkjRnzdayG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "config.py"
      ],
      "metadata": {
        "id": "EkrtlMbBdl0E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AsxKZkOdQnI"
      },
      "outputs": [],
      "source": [
        "REDDIT_CLIENT_ID = 'ihnchMgt0PaUYf2EQ1Rwnw'\n",
        "REDDIT_CLIENT_SECRET = '59C93P3TgpEvdx9-MOgPDxp90DUuKQ'\n",
        "REDDIT_USER_AGENT = 'TopicResearchBot by /u/East_Ambassador7130'\n",
        "\n",
        "TWITTER_BEARER_TOKEN = 'AAAAAAAAAAAAAAAAAAAAAFx73AEAAAAAE4I8alWZj3%2BLnXzlTBGNDE4LYwk%3D8SfCEv3TGtnQc3oKnmcj7kNe5BBMGjaQcn5nxMmZ8lYKzlzGrOn'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "credentials.json"
      ],
      "metadata": {
        "id": "XiYvEA2kd5rp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"type\": \"service_account\",\n",
        "  \"project_id\": \"reddit-twitter-research-bot\",\n",
        "  \"private_key_id\": \"c1a2b3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t0\",\n",
        "  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC7VhZQ.....f/1YQ==\\n-----END PRIVATE KEY-----\\n\",\n",
        "  \"client_email\": \"reddit-bot@reddit-twitter-research-bot.iam.gserviceaccount.com\",\n",
        "  \"client_id\": \"123456789012345678901\",\n",
        "  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
        "  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
        "  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
        "  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/reddit-bot%40reddit-twitter-research-bot.iam.gserviceaccount.com\"\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "7B9heNyreDp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "main.py"
      ],
      "metadata": {
        "id": "BVNFUOiseK83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import speech_recognition as sr\n",
        "import praw\n",
        "import requests\n",
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "from config import REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, REDDIT_USER_AGENT, TWITTER_BEARER_TOKEN\n",
        "\n",
        "def get_text_input():\n",
        "    choice = input(\"Enter 1 for voice input or 2 for text input: \")\n",
        "    if choice == '1':\n",
        "        r = sr.Recognizer()\n",
        "        with sr.Microphone() as source:\n",
        "            print(\"Speak your query...\")\n",
        "            audio = r.listen(source)\n",
        "        try:\n",
        "            return r.recognize_google(audio)\n",
        "        except sr.UnknownValueError:\n",
        "            print(\"Could not understand audio\")\n",
        "            return \"\"\n",
        "    else:\n",
        "        return input(\"Enter your query: \")\n",
        "\n",
        "def search_reddit(query, limit=5):\n",
        "    reddit = praw.Reddit(client_id=REDDIT_CLIENT_ID,\n",
        "                         client_secret=REDDIT_CLIENT_SECRET,\n",
        "                         user_agent=REDDIT_USER_AGENT)\n",
        "\n",
        "    results = []\n",
        "    for submission in reddit.subreddit(\"all\").search(query, limit=limit):\n",
        "        results.append({\n",
        "            \"Platform\": \"Reddit\",\n",
        "            \"Title\": submission.title,\n",
        "            \"URL\": submission.url,\n",
        "            \"Engagement\": submission.num_comments\n",
        "        })\n",
        "    return results\n",
        "\n",
        "def search_twitter(query, limit=5):\n",
        "    headers = {\"Authorization\": f\"Bearer {TWITTER_BEARER_TOKEN}\"}\n",
        "    search_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
        "    params = {\n",
        "        \"query\": query,\n",
        "        \"max_results\": limit,\n",
        "        \"tweet.fields\": \"public_metrics\"\n",
        "    }\n",
        "\n",
        "    response = requests.get(search_url, headers=headers, params=params)\n",
        "    tweets = response.json().get(\"data\", [])\n",
        "    results = []\n",
        "    for tweet in tweets:\n",
        "        metrics = tweet[\"public_metrics\"]\n",
        "        engagement = metrics[\"retweet_count\"] + metrics[\"reply_count\"] + metrics[\"like_count\"]\n",
        "        results.append({\n",
        "            \"Platform\": \"Twitter\",\n",
        "            \"Title\": tweet[\"text\"],\n",
        "            \"URL\": f\"https://twitter.com/i/web/status/{tweet['id']}\",\n",
        "            \"Engagement\": engagement\n",
        "        })\n",
        "    return results\n",
        "\n",
        "def save_to_google_sheets(data, sheet_name=\"Reddit & Twitter Results\"):\n",
        "    scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
        "    creds = ServiceAccountCredentials.from_json_keyfile_name(\"credentials.json\", scope)\n",
        "    client = gspread.authorize(creds)\n",
        "\n",
        "    try:\n",
        "        sheet = client.open(sheet_name).sheet1\n",
        "    except:\n",
        "        sheet = client.create(sheet_name).sheet1\n",
        "\n",
        "    sheet.clear()\n",
        "    sheet.append_row([\"Platform\", \"Title\", \"URL\", \"Engagement\"])\n",
        "    for item in data:\n",
        "        sheet.append_row([item[\"Platform\"], item[\"Title\"], item[\"URL\"], item[\"Engagement\"]])\n",
        "\n",
        "def main():\n",
        "    query = get_text_input()\n",
        "    if not query:\n",
        "        return\n",
        "\n",
        "    print(f\"\\nüîç Searching for: {query}\")\n",
        "    reddit_results = search_reddit(query)\n",
        "    twitter_results = search_twitter(query)\n",
        "    all_results = reddit_results + twitter_results\n",
        "\n",
        "    save_to_google_sheets(all_results)\n",
        "    print(\"\\n Results saved to Google Sheets.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "D6yIJNtmevBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2"
      ],
      "metadata": {
        "id": "9CFIopq6e06q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "amazon_scraper.py"
      ],
      "metadata": {
        "id": "syx_Rzdpe4lT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36\"\n",
        "}\n",
        "\n",
        "def get_amazon_products(keyword, pages=1):\n",
        "    all_products = []\n",
        "    for page in range(1, pages + 1):\n",
        "        url = f\"https://www.amazon.in/s?k={keyword}&page={page}\"\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        results = soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"})\n",
        "        for item in results:\n",
        "            try:\n",
        "                if \"Sponsored\" not in item.text:\n",
        "                    continue\n",
        "                title = item.h2.text.strip()\n",
        "                url = \"https://www.amazon.in\" + item.h2.a['href']\n",
        "                brand = item.find(\"h5\")\n",
        "                brand = brand.text.strip() if brand else \"Unknown\"\n",
        "                rating = item.find(\"span\", class_=\"a-icon-alt\")\n",
        "                rating = float(rating.text.split()[0]) if rating else None\n",
        "                reviews = item.find(\"span\", {\"class\": \"a-size-base\"})\n",
        "                reviews = int(reviews.text.replace(\",\", \"\")) if reviews else 0\n",
        "                price = item.find(\"span\", class_=\"a-price-whole\")\n",
        "                price = float(price.text.replace(\",\", \"\")) if price else None\n",
        "                image = item.find(\"img\")[\"src\"]\n",
        "                all_products.append({\n",
        "                    \"Title\": title,\n",
        "                    \"Brand\": brand,\n",
        "                    \"Rating\": rating,\n",
        "                    \"Reviews\": reviews,\n",
        "                    \"Price\": price,\n",
        "                    \"Image_URL\": image,\n",
        "                    \"Product_URL\": url\n",
        "                })\n",
        "            except Exception as e:\n",
        "                continue\n",
        "        time.sleep(2)\n",
        "    return pd.DataFrame(all_products)\n",
        "\n",
        "df = get_amazon_products(\"soft+toys\", pages=2)\n",
        "df.to_csv(\"soft_toys_raw.csv\", index=False)\n",
        "print(\"Scraping complete. File saved as soft_toys_raw.csv\")"
      ],
      "metadata": {
        "id": "rzlBxiFmfA0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "analysis.ipynb"
      ],
      "metadata": {
        "id": "ef1VCqCsfHAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Amazon Sponsored Soft Toys - Data Analysis\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load cleaned data\n",
        "df = pd.read_csv(\"cleaned_amazon_soft_toys.csv\")\n",
        "\n",
        "# --------- Brand Performance Analysis ---------\n",
        "# Count of each brand\n",
        "brand_counts = df['Brand'].value_counts().head(5)\n",
        "# Average rating by brand\n",
        "brand_ratings = df.groupby('Brand')['Rating'].mean().sort_values(ascending=False).head(5)\n",
        "\n",
        "# Bar chart: Top 5 brands by frequency\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=brand_counts.index, y=brand_counts.values, palette='viridis')\n",
        "plt.title('Top 5 Brands by Frequency')\n",
        "plt.xlabel('Brand')\n",
        "plt.ylabel('Number of Products')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Pie chart: Share of top 5 brands\n",
        "plt.figure(figsize=(8, 8))\n",
        "brand_counts.plot.pie(autopct='%1.1f%%', startangle=140, colors=sns.color_palette('pastel'))\n",
        "plt.title('Percentage Share of Top 5 Brands')\n",
        "plt.ylabel('')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --------- Price vs. Rating Analysis ---------\n",
        "# Scatter plot: Price vs Rating\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=df, x='Rating', y='Selling Price', hue='Brand', palette='Set2', legend=False)\n",
        "plt.title('Price vs Rating')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Price (INR)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Bar chart: Average price by rating range\n",
        "df['Rating Range'] = pd.cut(df['Rating'], bins=[0, 2, 3, 4, 5], labels=['0-2', '2-3', '3-4', '4-5'])\n",
        "avg_price_rating = df.groupby('Rating Range')['Selling Price'].mean()\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=avg_price_rating.index, y=avg_price_rating.values, palette='rocket')\n",
        "plt.title('Average Price by Rating Range')\n",
        "plt.xlabel('Rating Range')\n",
        "plt.ylabel('Avg Price (INR)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --------- Review & Rating Distribution ---------\n",
        "# Top 5 most reviewed products\n",
        "top_reviewed = df.sort_values('Reviews', ascending=False).head(5)\n",
        "# Top 5 highest-rated products\n",
        "top_rated = df.sort_values('Rating', ascending=False).head(5)\n",
        "\n",
        "# Bar chart: Top 5 most reviewed\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=top_reviewed['Title'], y=top_reviewed['Reviews'], palette='coolwarm')\n",
        "plt.title('Top 5 Most Reviewed Products')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.xlabel('Product Title')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Bar chart: Top 5 highest rated\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=top_rated['Title'], y=top_rated['Rating'], palette='mako')\n",
        "plt.title('Top 5 Highest Rated Products')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.xlabel('Product Title')\n",
        "plt.ylabel('Rating')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xAB-DClEfNJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "clean_data.py"
      ],
      "metadata": {
        "id": "Td9sV2GjfS0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"soft_toys_raw.csv\")\n",
        "df.drop_duplicates(inplace=True)\n",
        "df = df.dropna(subset=[\"Price\", \"Rating\"])\n",
        "df[\"Price\"] = pd.to_numeric(df[\"Price\"], errors='coerce')\n",
        "df[\"Rating\"] = pd.to_numeric(df[\"Rating\"], errors='coerce')\n",
        "df[\"Reviews\"] = pd.to_numeric(df[\"Reviews\"], errors='coerce')\n",
        "df.dropna(inplace=True)\n",
        "df.to_csv(\"soft_toys_cleaned.csv\", index=False)\n",
        "print(\"Cleaned data saved as soft_toys_cleaned.csv\")"
      ],
      "metadata": {
        "id": "ngM4Bnq3fcoE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}